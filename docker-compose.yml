services:
  # TRT Engine Builder - runs once to build the engine (FLUX.1-dev)
  trt-builder:
    build:
      context: .
      dockerfile: Dockerfile.fluxdev
    image: dippy-studio-bittensor-miner:fluxdev
    gpus: "all"
    entrypoint: ["/workspace/scripts/trt_build_in_container.sh"]
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # Build configuration (match your production settings)
      - TRT_BUILD_SAFE=0  # Set to 1 only if you need safe export mode
      - USE_FLASH_ATTENTION=1  # Enable for better performance
      - XFORMERS_DISABLED=0
      - MODEL_PATH=black-forest-labs/FLUX.1-dev
      - TRT_CACHE_DIR=/trt-cache
    volumes:
      - ./trt-cache:/trt-cache
      - .cache/huggingface:/root/.cache/huggingface
      - ./scripts:/workspace/scripts:ro
    profiles: ["build"]  # Doesn't start with regular docker compose up

  # TRT Engine Builder for FLUX.1-Kontext-dev
  trt-builder-kontext:
    build:
      context: .
      dockerfile: Dockerfile.kontext
    image: dippy-studio-bittensor-miner:kontext
    gpus: "all"
    entrypoint: ["/workspace/scripts/trt_build_in_container.sh"]
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # Build configuration (match your production settings)
      - TRT_BUILD_SAFE=0
      - USE_FLASH_ATTENTION=1
      - XFORMERS_DISABLED=0
      - MODEL_PATH=black-forest-labs/FLUX.1-Kontext-dev
      - TRT_CACHE_DIR=/trt-cache
    volumes:
      - ./trt-cache:/trt-cache
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./scripts:/workspace/scripts:ro
    profiles: ["build-kontext"]

  miner:
    build:
      context: .
      dockerfile: Dockerfile.fluxdev
    image: dippy-studio-bittensor-miner:fluxdev
    container_name: dippy-studio-bittensor-miner
    ports:
      - "8091:8091"
    gpus: "all"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Miner configuration
      - ENABLE_INFERENCE=${ENABLE_INFERENCE:-true}
      - ENABLE_KONTEXT_EDIT=${ENABLE_KONTEXT_EDIT:-false}
      - MODEL_PATH=${MODEL_PATH:-black-forest-labs/FLUX.1-dev}
      - KONTEXT_MODEL_PATH=${KONTEXT_MODEL_PATH:-black-forest-labs/FLUX.1-Kontext-dev}
      - LORA_PATH=${LORA_PATH:-}
      - OUTPUT_DIR=${OUTPUT_DIR:-/app/output}
      - MINER_SERVER_PORT=${MINER_SERVER_PORT:-8091}
      - MINER_SERVER_HOST=${MINER_SERVER_HOST:-0.0.0.0}
      - SERVICE_URL=${SERVICE_URL:-}  # Set to your public URL in production
      - TRT_ENGINE_PATH=/trt-cache/NVIDIA_H100_PCIe_cu12.4_trt10.5.0_fp16/transformer.plan
      - PYTHONHASHSEED=${PYTHONHASHSEED:-0}
      - CUBLAS_WORKSPACE_CONFIG=${CUBLAS_WORKSPACE_CONFIG:-:4096:8}

      # Runtime settings (no building in miner)
      - TRT_BUILD_SAFE=0
      - CUDA_FORCE_PTX_JIT=0  # Explicitly disable to prevent runtime errors
      - FAIL_ON_TRT_BUILD_ERROR=1  # Fail fast if engine missing
    volumes:
      - ./datasets:/app/datasets
      - ./output:/app/output
      - ./output:/workspace/output  # Working directory output mapping
      - ./config:/app/config
      - ./models:/app/models
      - ./trt-cache:/trt-cache
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped

  # Miner service for FLUX.1-Kontext-dev
  miner-kontext:
    build:
      context: .
      dockerfile: Dockerfile.kontext
    image: dippy-studio-bittensor-miner:kontext
    container_name: dippy-studio-bittensor-miner-kontext
    ports:
      - "8091:8091"
    gpus: "all"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Miner configuration
      - ENABLE_INFERENCE=${ENABLE_INFERENCE:-true}
      - ENABLE_KONTEXT_EDIT=${ENABLE_KONTEXT_EDIT:-true}
      - MODEL_PATH=${MODEL_PATH:-black-forest-labs/FLUX.1-Kontext-dev}
      - KONTEXT_MODEL_PATH=${KONTEXT_MODEL_PATH:-black-forest-labs/FLUX.1-Kontext-dev}
      - LORA_PATH=${LORA_PATH:-}
      - OUTPUT_DIR=${OUTPUT_DIR:-/app/output}
      - MINER_SERVER_PORT=${MINER_SERVER_PORT:-8091}
      - MINER_SERVER_HOST=${MINER_SERVER_HOST:-0.0.0.0}
      - SERVICE_URL=${SERVICE_URL:-}
      - TRT_ENGINE_PATH=/trt-cache/NVIDIA_H100_PCIe_cu12.4_trt10.5.0_fp16/transformer.plan
      - PYTHONHASHSEED=${PYTHONHASHSEED:-0}
      - CUBLAS_WORKSPACE_CONFIG=${CUBLAS_WORKSPACE_CONFIG:-:4096:8}

      # Runtime settings
      - TRT_BUILD_SAFE=0
      - CUDA_FORCE_PTX_JIT=0
      - FAIL_ON_TRT_BUILD_ERROR=1
    volumes:
      - ./datasets:/app/datasets
      - ./output:/app/output
      - ./output:/workspace/output
      - ./config:/app/config
      - ./models:/app/models
      - ./trt-cache:/trt-cache
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    profiles: ["kontext"]
